{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from subprocess import call\n",
    "import numpy as np\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the models\n",
    "\n",
    "As mentioned in the readme, here are the pretrained models you can download:\n",
    "\n",
    "- [sent2vec_wiki_unigrams](https://drive.google.com/open?id=0B6VhzidiLvjSa19uYWlLUEkzX3c) 5GB (600dim, trained on english wikipedia)\n",
    "- [sent2vec_wiki_bigrams](https://drive.google.com/open?id=0B6VhzidiLvjSaER5YkJUdWdPWU0) 16GB (700dim, trained on english wikipedia)\n",
    "- [sent2vec_twitter_unigrams](https://drive.google.com/open?id=0B6VhzidiLvjSaVFLM0xJNk9DTzg) 13GB (700dim, trained on english tweets)\n",
    "- [sent2vec_twitter_bigrams](https://drive.google.com/open?id=0B6VhzidiLvjSeHI4cmdQdXpTRHc) 23GB (700dim, trained on english tweets)\n",
    "- [sent2vec_toronto books_unigrams](https://drive.google.com/open?id=0B6VhzidiLvjSOWdGM0tOX1lUNEk) 2GB (700dim, trained on the [BookCorpus dataset](http://yknzhu.wixsite.com/mbweb))\n",
    "- [sent2vec_toronto books_bigrams](https://drive.google.com/open?id=0B6VhzidiLvjSdENLSEhrdWprQ0k) 7GB (700dim, trained on the [BookCorpus dataset](http://yknzhu.wixsite.com/mbweb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "From here, one simple way to get sentence embeddings is to use the `print-sentence-vectors` command as shown in the README.  To properly use our models you ideally need to use the same preprocessing used during training. We provide here some simple code wrapping around the `print-sentence-vectors` command and handling the tokenization to match our models properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking things together\n",
    "\n",
    "In order to use the Stanford NLP tokenizer with NLTK, you need to get the `stanford-postagger.jar` available in the [CoreNLP library package](http://stanfordnlp.github.io/CoreNLP/).\n",
    "\n",
    "You can then proceed to link things by modifying the paths in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_EXEC_PATH = os.path.abspath(\"./fasttext\")\n",
    "\n",
    "BASE_SNLP_PATH = \"/home/aires/repositories/sent2vec/models/stanford-corenlp-full-2017-06-09/\"\n",
    "SNLP_TAGGER_JAR = os.path.join(BASE_SNLP_PATH, \"stanford-corenlp-3.8.0.jar\")\n",
    "\n",
    "MODEL_WIKI_UNIGRAMS = os.path.abspath(\"models/wiki_unigrams.bin\")\n",
    "# MODEL_WIKI_BIGRAMS = os.path.abspath(\"./sent2vec_wiki_bigrams\")\n",
    "# MODEL_TORONTOBOOKS_UNIGRAMS = os.path.abspath(\"./sent2vec_wiki_unigrams\")\n",
    "# MODEL_TORONTOBOOKS_BIGRAMS = os.path.abspath(\"./sent2vec_wiki_bigrams\")\n",
    "# MODEL_TWITTER_UNIGRAMS = os.path.abspath('./sent2vec_twitter_unigrams')\n",
    "# MODEL_TWITTER_BIGRAMS = os.path.abspath('./sent2vec_twitter_bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating sentence embeddings\n",
    "\n",
    "Now you can just run the following cells:\n",
    "\n",
    "## Utils for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tknzr, sentence, to_lower=True):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))','<url>',sentence) #replace urls by <url>\n",
    "    sentence = re.sub('(\\@[^\\s]+)','<user>',sentence) #replace @user268 by <user>\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token\n",
    "\n",
    "def tokenize_sentences(tknzr, sentences, to_lower=True):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentences: a list of sentences\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    return [tokenize(tknzr, s, to_lower) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for inferring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_preprocessed_sentences(sentences, model_path, fasttext_exec_path):\n",
    "    \"\"\"Arguments:\n",
    "        - sentences: a list of preprocessed sentences\n",
    "        - model_path: a path to the sent2vec .bin model\n",
    "        - fasttext_exec_path: a path to the fasttext executable\n",
    "    \"\"\"\n",
    "    timestamp = str(time.time())\n",
    "#     print timestamp\n",
    "    test_path = os.path.abspath('./'+timestamp+'_fasttext.test.txt')\n",
    "    embeddings_path = os.path.abspath('./'+timestamp+'_fasttext.embeddings.txt')\n",
    "#     print test_path, embeddings_path\n",
    "    dump_text_to_disk(test_path, sentences)\n",
    "    call(fasttext_exec_path+\n",
    "          ' print-sentence-vectors '+\n",
    "          model_path + ' < '+\n",
    "          test_path + ' > ' +\n",
    "          embeddings_path, shell=True)\n",
    "    embeddings = read_embeddings(embeddings_path)\n",
    "    os.remove(test_path)\n",
    "    os.remove(embeddings_path)\n",
    "#     print len(embeddings), len(sentences)\n",
    "#     print embeddings, sentences\n",
    "    assert(len(sentences) == len(embeddings))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    \"\"\"Arguments:\n",
    "        - embeddings_path: path to the embeddings\n",
    "    \"\"\"\n",
    "#     print \"I'm reading here. \", embeddings_path\n",
    "    with open(embeddings_path, 'r') as in_stream:\n",
    "        embeddings = []\n",
    "        for line in in_stream:\n",
    "#             print line\n",
    "            line = '['+line.replace(' ',',')+']'\n",
    "            embeddings.append(eval(line))\n",
    "        return embeddings\n",
    "    return []\n",
    "\n",
    "def dump_text_to_disk(file_path, X, Y=None):\n",
    "    \"\"\"Arguments:\n",
    "        - file_path: where to dump the data\n",
    "        - X: list of sentences to dump\n",
    "        - Y: labels, if any\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as out_stream:\n",
    "#         print \"start writting here. y:\", Y\n",
    "        if Y is not None:\n",
    "            for x, y in zip(X, Y):\n",
    "                out_stream.write('__label__'+str(y)+' '+x+' \\n')\n",
    "        else:\n",
    "            for x in X:\n",
    "#                 print x\n",
    "                out_stream.write(x+' \\n')\n",
    "\n",
    "def get_sentence_embeddings(sentences, ngram='unigrams', model='concat_wiki_twitter'):\n",
    "    \"\"\" Returns a numpy matrix of embeddings for one of the published models. It\n",
    "    handles tokenization and can be given raw sentences.\n",
    "    Arguments:\n",
    "        - ngram: 'unigrams' or 'bigrams'\n",
    "        - model: 'wiki', 'twitter', or 'concat_wiki_twitter'\n",
    "        - sentences: a list of raw sentences ['Once upon a time', 'This is another sentence.', ...]\n",
    "    \"\"\"\n",
    "    wiki_embeddings = None\n",
    "    twitter_embbedings = None\n",
    "    tokenized_sentences_NLTK_tweets = None\n",
    "    tokenized_sentences_SNLP = None\n",
    "    if model == \"wiki\" or model == 'concat_wiki_twitter':\n",
    "        tknzr = StanfordTokenizer(SNLP_TAGGER_JAR, encoding='utf-8')\n",
    "        s = ' <delimiter> '.join(sentences) #just a trick to make things faster\n",
    "        tokenized_sentences_SNLP = tokenize_sentences(tknzr, [s])\n",
    "        tokenized_sentences_SNLP = tokenized_sentences_SNLP[0].split(' <delimiter> ')\n",
    "        assert(len(tokenized_sentences_SNLP) == len(sentences))\n",
    "        if ngram == 'unigrams':\n",
    "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
    "                                     MODEL_WIKI_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "        else:\n",
    "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
    "                                     MODEL_WIKI_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "    if model == \"twitter\" or model == 'concat_wiki_twitter':\n",
    "        tknzr = TweetTokenizer()\n",
    "        tokenized_sentences_NLTK_tweets = tokenize_sentences(tknzr, sentences)\n",
    "        if ngram == 'unigrams':\n",
    "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
    "                                     MODEL_TWITTER_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "        else:\n",
    "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
    "                                     MODEL_TWITTER_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "    if model == \"twitter\":\n",
    "        return twitter_embbedings\n",
    "    elif model == \"wiki\":\n",
    "        return wiki_embeddings\n",
    "    elif model == \"concat_wiki_twitter\":\n",
    "        return np.concatenate((wiki_embeddings, twitter_embbedings), axis=1)\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecase\n",
    "\n",
    "To get embeddings you can now use the `get_sentence_embeddings` function, the paremeters are:\n",
    "- sentences: a list of unprocessed sentences\n",
    "- ngram: either `bigrams` or `unigrams`\n",
    "- model: `wiki`, `twitter` or `concat_wiki_twitter`\n",
    "\n",
    "Loading the models can take some time, but once loaded the inferrence is fast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Get norm sentence paths.\n",
    "conflicts_path = '/home/aires/datasets/conflicts/conflicts.csv'\n",
    "non_conflicts_path = '/home/aires/datasets/conflicts/non-conflicts.csv'\n",
    "CONFLICT = 1\n",
    "N_CONFLICT = 0\n",
    "\n",
    "norms_seen = []\n",
    "counter = 0\n",
    "cnflcts = []\n",
    "nn_cnflcts = []\n",
    "y = []\n",
    "\n",
    "# Read conflicts.\n",
    "df_conflict = pd.read_csv(conflicts_path)\n",
    "\n",
    "conf_rows = len(df_conflict) # Get the number of conflicting samples.\n",
    "\n",
    "for i in range(conf_rows):\n",
    "    # Get norm pair.\n",
    "    norm1, norm2 = df_conflict['norm1'][i], df_conflict['norm2'][i]\n",
    "    \n",
    "    cnflcts.append((norm1, norm2))\n",
    "    y.append(CONFLICT) # Mark the pair as a conflicting one.\n",
    "    \n",
    "    if norm1 not in norms_seen:\n",
    "        # Avoid adding duplicates.\n",
    "        norms_seen.append(norm1)\n",
    "    if norm2 not in norms_seen:\n",
    "        norms_seen.append(norm2)\n",
    "\n",
    "# Read non-conflicts.\n",
    "df_non_conflict = pd.read_csv(non_conflicts_path)\n",
    "\n",
    "non_conf_rows = len(df_non_conflict)\n",
    "\n",
    "for i in range(conf_rows):\n",
    "    \n",
    "    j = random.randint(0, non_conf_rows) # Get a random pair of non-conflicting norms.\n",
    "    \n",
    "    norm1, norm2 = df_non_conflict['norm1'][j], df_non_conflict['norm2'][j]\n",
    "    \n",
    "    nn_cnflcts.append((norm1, norm2))\n",
    "    y.append(N_CONFLICT) # Mark the pair as a non-conflicting one.\n",
    "    \n",
    "    if norm1 not in norms_seen:\n",
    "        norms_seen.append(norm1)\n",
    "    if norm2 not in norms_seen:\n",
    "        norms_seen.append(norm2)\n",
    "        \n",
    "norms = dict() # Create a dictionary to acess norms addresing an index for each one.\n",
    "for i, x in enumerate(norms_seen):\n",
    "    norms[x] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPTokenizer\u001b[0m instead.'\n"
     ]
    }
   ],
   "source": [
    "# Generating embeddings\n",
    "embeddings = get_sentence_embeddings(norms_seen, ngram='unigrams', model='wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folds.\n",
    "from sklearn.model_selection import KFold\n",
    "random_st = 32\n",
    "n_folds = [7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset(train, train_indexes):\n",
    "    \n",
    "    # Create offset.\n",
    "    embedding_sum = np.zeros(embeddings.shape[1]) # Create and empty array to receive the sum of all\n",
    "                                                  # conflicting embeddings.\n",
    "    for i in train_indexes:\n",
    "        # Sum all embeddings to one.\n",
    "        embedding_sum += embeddings[norms[train[i][0]]] - embeddings[norms[train[i][1]]]\n",
    "\n",
    "    # Take the mean to obtain the offset.\n",
    "    return embedding_sum / train_indexes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tUsing a total of 7 folds.\n",
      "Working on fold 0.\n",
      "Accuracy: 0.93\n",
      "Working on fold 1.\n",
      "Accuracy: 0.97\n",
      "Working on fold 2.\n",
      "Accuracy: 0.93\n",
      "Working on fold 3.\n",
      "Accuracy: 0.97\n",
      "Working on fold 4.\n",
      "Accuracy: 0.97\n",
      "Working on fold 5.\n",
      "Accuracy: 0.90\n",
      "Working on fold 6.\n",
      "Accuracy: 0.96\n",
      "Mean accuracy: 0.95\n",
      "\t\tUsing a total of 8 folds.\n",
      "Working on fold 7.\n",
      "Accuracy: 0.96\n",
      "Working on fold 8.\n",
      "Accuracy: 0.96\n",
      "Working on fold 9.\n",
      "Accuracy: 0.92\n",
      "Working on fold 10.\n",
      "Accuracy: 0.96\n",
      "Working on fold 11.\n",
      "Accuracy: 0.96\n",
      "Working on fold 12.\n",
      "Accuracy: 0.96\n",
      "Working on fold 13.\n",
      "Accuracy: 0.88\n",
      "Working on fold 14.\n",
      "Accuracy: 0.96\n",
      "Mean accuracy: 0.95\n",
      "\t\tUsing a total of 9 folds.\n",
      "Working on fold 15.\n",
      "Accuracy: 0.96\n",
      "Working on fold 16.\n",
      "Accuracy: 0.96\n",
      "Working on fold 17.\n",
      "Accuracy: 0.96\n",
      "Working on fold 18.\n",
      "Accuracy: 0.92\n",
      "Working on fold 19.\n",
      "Accuracy: 0.96\n",
      "Working on fold 20.\n",
      "Accuracy: 0.95\n",
      "Working on fold 21.\n",
      "Accuracy: 0.95\n",
      "Working on fold 22.\n",
      "Accuracy: 0.91\n",
      "Working on fold 23.\n",
      "Accuracy: 0.95\n",
      "Mean accuracy: 0.95\n",
      "\t\tUsing a total of 10 folds.\n",
      "Working on fold 24.\n",
      "Accuracy: 0.95\n",
      "Working on fold 25.\n",
      "Accuracy: 0.95\n",
      "Working on fold 26.\n",
      "Accuracy: 0.95\n",
      "Working on fold 27.\n",
      "Accuracy: 0.91\n",
      "Working on fold 28.\n",
      "Accuracy: 0.95\n",
      "Working on fold 29.\n",
      "Accuracy: 1.00\n",
      "Working on fold 30.\n",
      "Accuracy: 0.95\n",
      "Working on fold 31.\n",
      "Accuracy: 0.95\n",
      "Working on fold 32.\n",
      "Accuracy: 0.90\n",
      "Working on fold 33.\n",
      "Accuracy: 0.95\n",
      "Mean accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Making a test over real conflicts.\n",
    "fold = 0\n",
    "\n",
    "for n_fold in n_folds:\n",
    "    # Run over the number of folds.\n",
    "    acc_sum = 0\n",
    "    \n",
    "    kf = KFold(n_splits=n_fold, shuffle=True, random_state=random_st)\n",
    "    \n",
    "    print \"\\t\\tUsing a total of %d folds.\" % n_fold\n",
    "    \n",
    "    for train_index, test_index in kf.split(cnflcts):\n",
    "\n",
    "        y_gold = []\n",
    "        y_pred = []\n",
    "\n",
    "        print \"Working on fold %d.\" % fold\n",
    "        fold += 1\n",
    "\n",
    "        # Generating offset for this fold.\n",
    "        offset = generate_offset(cnflcts, train_index)\n",
    "\n",
    "        for i in test_index:\n",
    "            # Create diffs for conflicts.\n",
    "            y_gold.append(CONFLICT)\n",
    "            diff = embeddings[norms[cnflcts[i][0]]] - embeddings[norms[cnflcts[i][1]]]\n",
    "            conflict_diff = np.linalg.norm(offset - diff)\n",
    "\n",
    "            if conflict_diff < threshold:\n",
    "                y_pred.append(CONFLICT)\n",
    "            else:\n",
    "                y_pred.append(N_CONFLICT)\n",
    "\n",
    "            j = random.randint(0, len(nn_cnflcts) - 1)\n",
    "\n",
    "            diff = embeddings[norms[nn_cnflcts[j][0]]] - embeddings[norms[nn_cnflcts[j][1]]]\n",
    "            conflict_diff = np.linalg.norm(offset - diff)\n",
    "            y_gold.append(N_CONFLICT)\n",
    "\n",
    "            if conflict_diff < threshold:\n",
    "                y_pred.append(CONFLICT)\n",
    "            else:\n",
    "                y_pred.append(N_CONFLICT)\n",
    "\n",
    "        acc = accuracy_score(y_gold, y_pred)\n",
    "        print \"Accuracy: %.2f\" % acc\n",
    "        acc_sum += acc\n",
    "\n",
    "    mean_acc = acc_sum / n_fold\n",
    "    print \"Mean accuracy: %.2f\" % mean_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
