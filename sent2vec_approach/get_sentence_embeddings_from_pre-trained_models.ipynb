{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from subprocess import call\n",
    "import numpy as np\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the models\n",
    "\n",
    "As mentioned in the readme, here are the pretrained models you can download:\n",
    "\n",
    "- [sent2vec_wiki_unigrams](https://drive.google.com/open?id=0B6VhzidiLvjSa19uYWlLUEkzX3c) 5GB (600dim, trained on english wikipedia)\n",
    "- [sent2vec_wiki_bigrams](https://drive.google.com/open?id=0B6VhzidiLvjSaER5YkJUdWdPWU0) 16GB (700dim, trained on english wikipedia)\n",
    "- [sent2vec_twitter_unigrams](https://drive.google.com/open?id=0B6VhzidiLvjSaVFLM0xJNk9DTzg) 13GB (700dim, trained on english tweets)\n",
    "- [sent2vec_twitter_bigrams](https://drive.google.com/open?id=0B6VhzidiLvjSeHI4cmdQdXpTRHc) 23GB (700dim, trained on english tweets)\n",
    "- [sent2vec_toronto books_unigrams](https://drive.google.com/open?id=0B6VhzidiLvjSOWdGM0tOX1lUNEk) 2GB (700dim, trained on the [BookCorpus dataset](http://yknzhu.wixsite.com/mbweb))\n",
    "- [sent2vec_toronto books_bigrams](https://drive.google.com/open?id=0B6VhzidiLvjSdENLSEhrdWprQ0k) 7GB (700dim, trained on the [BookCorpus dataset](http://yknzhu.wixsite.com/mbweb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code provided by Pagliardini et al.\n",
    "For more details, please visit authors' [repository](https://github.com/epfml/sent2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_EXEC_PATH = os.path.abspath(\"./fasttext\")\n",
    "\n",
    "BASE_SNLP_PATH = \"/home/aires/repositories/sent2vec/models/stanford-corenlp-full-2017-06-09/\"\n",
    "SNLP_TAGGER_JAR = os.path.join(BASE_SNLP_PATH, \"stanford-corenlp-3.8.0.jar\")\n",
    "\n",
    "MODEL_WIKI_UNIGRAMS = os.path.abspath(\"models/wiki_unigrams.bin\")\n",
    "# MODEL_WIKI_BIGRAMS = os.path.abspath(\"./sent2vec_wiki_bigrams\")\n",
    "# MODEL_TORONTOBOOKS_UNIGRAMS = os.path.abspath(\"./sent2vec_wiki_unigrams\")\n",
    "# MODEL_TORONTOBOOKS_BIGRAMS = os.path.abspath(\"./sent2vec_wiki_bigrams\")\n",
    "# MODEL_TWITTER_UNIGRAMS = os.path.abspath('./sent2vec_twitter_unigrams')\n",
    "# MODEL_TWITTER_BIGRAMS = os.path.abspath('./sent2vec_twitter_bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating sentence embeddings\n",
    "\n",
    "Now you can just run the following cells:\n",
    "\n",
    "## Utils for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tknzr, sentence, to_lower=True):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))','<url>',sentence) #replace urls by <url>\n",
    "    sentence = re.sub('(\\@[^\\s]+)','<user>',sentence) #replace @user268 by <user>\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token\n",
    "\n",
    "def tokenize_sentences(tknzr, sentences, to_lower=True):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentences: a list of sentences\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    return [tokenize(tknzr, s, to_lower) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for inferring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_preprocessed_sentences(sentences, model_path, fasttext_exec_path):\n",
    "    \"\"\"Arguments:\n",
    "        - sentences: a list of preprocessed sentences\n",
    "        - model_path: a path to the sent2vec .bin model\n",
    "        - fasttext_exec_path: a path to the fasttext executable\n",
    "    \"\"\"\n",
    "    timestamp = str(time.time())\n",
    "#     print timestamp\n",
    "    test_path = os.path.abspath('./'+timestamp+'_fasttext.test.txt')\n",
    "    embeddings_path = os.path.abspath('./'+timestamp+'_fasttext.embeddings.txt')\n",
    "#     print test_path, embeddings_path\n",
    "    dump_text_to_disk(test_path, sentences)\n",
    "    call(fasttext_exec_path+\n",
    "          ' print-sentence-vectors '+\n",
    "          model_path + ' < '+\n",
    "          test_path + ' > ' +\n",
    "          embeddings_path, shell=True)\n",
    "    embeddings = read_embeddings(embeddings_path)\n",
    "    os.remove(test_path)\n",
    "    os.remove(embeddings_path)\n",
    "#     print len(embeddings), len(sentences)\n",
    "#     print embeddings, sentences\n",
    "    assert(len(sentences) == len(embeddings))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    \"\"\"Arguments:\n",
    "        - embeddings_path: path to the embeddings\n",
    "    \"\"\"\n",
    "#     print \"I'm reading here. \", embeddings_path\n",
    "    with open(embeddings_path, 'r') as in_stream:\n",
    "        embeddings = []\n",
    "        for line in in_stream:\n",
    "#             print line\n",
    "            line = '['+line.replace(' ',',')+']'\n",
    "            embeddings.append(eval(line))\n",
    "        return embeddings\n",
    "    return []\n",
    "\n",
    "def dump_text_to_disk(file_path, X, Y=None):\n",
    "    \"\"\"Arguments:\n",
    "        - file_path: where to dump the data\n",
    "        - X: list of sentences to dump\n",
    "        - Y: labels, if any\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as out_stream:\n",
    "#         print \"start writting here. y:\", Y\n",
    "        if Y is not None:\n",
    "            for x, y in zip(X, Y):\n",
    "                out_stream.write('__label__'+str(y)+' '+x+' \\n')\n",
    "        else:\n",
    "            for x in X:\n",
    "#                 print x\n",
    "                out_stream.write(x+' \\n')\n",
    "\n",
    "def get_sentence_embeddings(sentences, ngram='unigrams', model='concat_wiki_twitter'):\n",
    "    \"\"\" Returns a numpy matrix of embeddings for one of the published models. It\n",
    "    handles tokenization and can be given raw sentences.\n",
    "    Arguments:\n",
    "        - ngram: 'unigrams' or 'bigrams'\n",
    "        - model: 'wiki', 'twitter', or 'concat_wiki_twitter'\n",
    "        - sentences: a list of raw sentences ['Once upon a time', 'This is another sentence.', ...]\n",
    "    \"\"\"\n",
    "    wiki_embeddings = None\n",
    "    twitter_embbedings = None\n",
    "    tokenized_sentences_NLTK_tweets = None\n",
    "    tokenized_sentences_SNLP = None\n",
    "    if model == \"wiki\" or model == 'concat_wiki_twitter':\n",
    "        tknzr = StanfordTokenizer(SNLP_TAGGER_JAR, encoding='utf-8')\n",
    "        s = ' <delimiter> '.join(sentences) #just a trick to make things faster\n",
    "        tokenized_sentences_SNLP = tokenize_sentences(tknzr, [s])\n",
    "        tokenized_sentences_SNLP = tokenized_sentences_SNLP[0].split(' <delimiter> ')\n",
    "        assert(len(tokenized_sentences_SNLP) == len(sentences))\n",
    "        if ngram == 'unigrams':\n",
    "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
    "                                     MODEL_WIKI_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "        else:\n",
    "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
    "                                     MODEL_WIKI_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "    if model == \"twitter\" or model == 'concat_wiki_twitter':\n",
    "        tknzr = TweetTokenizer()\n",
    "        tokenized_sentences_NLTK_tweets = tokenize_sentences(tknzr, sentences)\n",
    "        if ngram == 'unigrams':\n",
    "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
    "                                     MODEL_TWITTER_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "        else:\n",
    "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
    "                                     MODEL_TWITTER_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
    "    if model == \"twitter\":\n",
    "        return twitter_embbedings\n",
    "    elif model == \"wiki\":\n",
    "        return wiki_embeddings\n",
    "    elif model == \"concat_wiki_twitter\":\n",
    "        return np.concatenate((wiki_embeddings, twitter_embbedings), axis=1)\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norm Conflict Identification\n",
    "\n",
    "In this section of the code we adapt to use sent2vec on the norm conflict identification.\n",
    "\n",
    "Notice that you need to change the path of conflicts_path and non_conflicts_path in order to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "conflicts_path = '/home/aires/datasets/conflicts/conflicts.csv'\n",
    "non_conflicts_path = '/home/aires/datasets/conflicts/non-conflicts.csv'\n",
    "CONFLICT = 1\n",
    "N_CONFLICT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /home/aires/datasets/conflicts/conflicts.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-54c46d00c72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Read conflicts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_conflict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconflicts_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconf_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_conflict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get the number of conflicting samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aires/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aires/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aires/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aires/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aires/.local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /home/aires/datasets/conflicts/conflicts.csv does not exist"
     ]
    }
   ],
   "source": [
    "norms_seen = [] # Account new norms.\n",
    "cnflcts = []\n",
    "nn_cnflcts = []\n",
    "y = [] # Norm pair classes (Either 1 for conflicts or 0 for non-conflicts).\n",
    "\n",
    "# Read conflicts.\n",
    "df_conflict = pd.read_csv(conflicts_path)\n",
    "\n",
    "conf_rows = len(df_conflict) # Get the number of conflicting samples.\n",
    "\n",
    "for i in range(conf_rows):\n",
    "    # Get norm pair.\n",
    "    norm1, norm2 = df_conflict['norm1'][i], df_conflict['norm2'][i]\n",
    "    \n",
    "    cnflcts.append((norm1, norm2)) # Save the conflicting pair.\n",
    "    y.append(CONFLICT) # Mark the pair as a conflicting one.\n",
    "    \n",
    "    if norm1 not in norms_seen:\n",
    "        # Avoid adding duplicates.\n",
    "        norms_seen.append(norm1)\n",
    "    if norm2 not in norms_seen:\n",
    "        norms_seen.append(norm2)\n",
    "\n",
    "# Read non-conflicts.\n",
    "df_non_conflict = pd.read_csv(non_conflicts_path)\n",
    "\n",
    "non_conf_rows = len(df_non_conflict)\n",
    "\n",
    "for i in range(conf_rows):\n",
    "    \n",
    "    j = random.randint(0, non_conf_rows) # Get a random pair of non-conflicting norms.\n",
    "    # We select a random non conflict pair because we have many more non-conflicting cases than conflicts.\n",
    "    \n",
    "    norm1, norm2 = df_non_conflict['norm1'][j], df_non_conflict['norm2'][j]\n",
    "    \n",
    "    nn_cnflcts.append((norm1, norm2))\n",
    "    y.append(N_CONFLICT) # Mark the pair as a non-conflicting one.\n",
    "    \n",
    "    if norm1 not in norms_seen:\n",
    "        norms_seen.append(norm1)\n",
    "    if norm2 not in norms_seen:\n",
    "        norms_seen.append(norm2)\n",
    "        \n",
    "norms = dict() # Create a dictionary to acess norms addresing an index for each one.\n",
    "for i, x in enumerate(norms_seen):\n",
    "    norms[x] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPTokenizer\u001b[0m instead.'\n"
     ]
    }
   ],
   "source": [
    "# Generating embeddings\n",
    "embeddings = get_sentence_embeddings(norms_seen, ngram='unigrams', model='wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folds.\n",
    "from sklearn.model_selection import KFold\n",
    "random_st = 32\n",
    "n_folds = [7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset(train, train_indexes):    \n",
    "    # Create offset.\n",
    "    embedding_sum = np.zeros(embeddings.shape[1]) # Create and empty array to receive the sum of all\n",
    "                                                  # conflicting embeddings.\n",
    "    for i in train_indexes:\n",
    "        # Sum all embeddings to one.\n",
    "        embedding_sum += embeddings[norms[train[i][0]]] - embeddings[norms[train[i][1]]]\n",
    "\n",
    "    # Take the mean to obtain the offset.\n",
    "    return embedding_sum / train_indexes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tUsing a total of 7 folds.\n",
      "Working on fold 0.\n",
      "Accuracy: 0.93\n",
      "Working on fold 1.\n",
      "Accuracy: 0.97\n",
      "Working on fold 2.\n",
      "Accuracy: 0.93\n",
      "Working on fold 3.\n",
      "Accuracy: 0.97\n",
      "Working on fold 4.\n",
      "Accuracy: 0.97\n",
      "Working on fold 5.\n",
      "Accuracy: 0.90\n",
      "Working on fold 6.\n",
      "Accuracy: 0.96\n",
      "Mean accuracy: 0.95\n",
      "\t\tUsing a total of 8 folds.\n",
      "Working on fold 7.\n",
      "Accuracy: 0.96\n",
      "Working on fold 8.\n",
      "Accuracy: 0.96\n",
      "Working on fold 9.\n",
      "Accuracy: 0.92\n",
      "Working on fold 10.\n",
      "Accuracy: 0.96\n",
      "Working on fold 11.\n",
      "Accuracy: 0.96\n",
      "Working on fold 12.\n",
      "Accuracy: 0.96\n",
      "Working on fold 13.\n",
      "Accuracy: 0.88\n",
      "Working on fold 14.\n",
      "Accuracy: 0.96\n",
      "Mean accuracy: 0.95\n",
      "\t\tUsing a total of 9 folds.\n",
      "Working on fold 15.\n",
      "Accuracy: 0.96\n",
      "Working on fold 16.\n",
      "Accuracy: 0.96\n",
      "Working on fold 17.\n",
      "Accuracy: 0.96\n",
      "Working on fold 18.\n",
      "Accuracy: 0.92\n",
      "Working on fold 19.\n",
      "Accuracy: 0.96\n",
      "Working on fold 20.\n",
      "Accuracy: 0.95\n",
      "Working on fold 21.\n",
      "Accuracy: 0.95\n",
      "Working on fold 22.\n",
      "Accuracy: 0.91\n",
      "Working on fold 23.\n",
      "Accuracy: 0.95\n",
      "Mean accuracy: 0.95\n",
      "\t\tUsing a total of 10 folds.\n",
      "Working on fold 24.\n",
      "Accuracy: 0.95\n",
      "Working on fold 25.\n",
      "Accuracy: 0.95\n",
      "Working on fold 26.\n",
      "Accuracy: 0.95\n",
      "Working on fold 27.\n",
      "Accuracy: 0.91\n",
      "Working on fold 28.\n",
      "Accuracy: 0.95\n",
      "Working on fold 29.\n",
      "Accuracy: 1.00\n",
      "Working on fold 30.\n",
      "Accuracy: 0.95\n",
      "Working on fold 31.\n",
      "Accuracy: 0.95\n",
      "Working on fold 32.\n",
      "Accuracy: 0.90\n",
      "Working on fold 33.\n",
      "Accuracy: 0.95\n",
      "Mean accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Making a test over real conflicts.\n",
    "fold = 0\n",
    "\n",
    "for n_fold in n_folds:\n",
    "    # Run over the number of folds.\n",
    "    acc_sum = 0\n",
    "    \n",
    "    kf = KFold(n_splits=n_fold, shuffle=True, random_state=random_st)\n",
    "    \n",
    "    print \"\\t\\tUsing a total of %d folds.\" % n_fold\n",
    "    \n",
    "    for train_index, test_index in kf.split(cnflcts):\n",
    "\n",
    "        y_gold = []\n",
    "        y_pred = []\n",
    "\n",
    "        print \"Working on fold %d.\" % fold\n",
    "        fold += 1\n",
    "\n",
    "        # Generating offset for this fold.\n",
    "        offset = generate_offset(cnflcts, train_index)\n",
    "\n",
    "        for i in test_index:\n",
    "            # Create diffs for conflicts.\n",
    "            y_gold.append(CONFLICT)\n",
    "            diff = embeddings[norms[cnflcts[i][0]]] - embeddings[norms[cnflcts[i][1]]]\n",
    "            conflict_diff = np.linalg.norm(offset - diff)\n",
    "\n",
    "            if conflict_diff < threshold:\n",
    "                y_pred.append(CONFLICT)\n",
    "            else:\n",
    "                y_pred.append(N_CONFLICT)\n",
    "\n",
    "            j = random.randint(0, len(nn_cnflcts) - 1)\n",
    "\n",
    "            diff = embeddings[norms[nn_cnflcts[j][0]]] - embeddings[norms[nn_cnflcts[j][1]]]\n",
    "            conflict_diff = np.linalg.norm(offset - diff)\n",
    "            y_gold.append(N_CONFLICT)\n",
    "\n",
    "            if conflict_diff < threshold:\n",
    "                y_pred.append(CONFLICT)\n",
    "            else:\n",
    "                y_pred.append(N_CONFLICT)\n",
    "\n",
    "        acc = accuracy_score(y_gold, y_pred)\n",
    "        print \"Accuracy: %.2f\" % acc\n",
    "        acc_sum += acc\n",
    "\n",
    "    mean_acc = acc_sum / n_fold\n",
    "    print \"Mean accuracy: %.2f\" % mean_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
