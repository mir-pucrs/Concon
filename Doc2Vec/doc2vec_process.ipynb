{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Process\n",
    "\n",
    "We divide this process into three steps:\n",
    "\n",
    "- **Doc2Vec Model Trainng**: Using contracts, we train a doc2vec model to turn contract sentences into representations.\n",
    "\n",
    "- **Processing a New Contract**: Given the doc2vec model, we start the process in a new contract.\n",
    "\n",
    "    - Norm Extraction: First, we extract the norms from the new contract;\n",
    "    - Then, we create a representation for each norm using the doc2vec model.\n",
    "    \n",
    "- **Conflict Identification**: Using the norm representations, we can have two different paths to follow:\n",
    "\n",
    "    - T-SNE: Manual identification of modal verbs. (Experimental)\n",
    "    - Norm Comparisons: Compare norms and find the most similar among them based on a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aires/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "from random import shuffle\n",
    "from convert_to_sentences import convert_to_sentences\n",
    "from time import gmtime, strftime\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sentence_classifier.sentence_classifier import SentenceClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONSTANTS.\n",
    "TRAIN = True\n",
    "TRAIN_PATH = 'dataset/manufact_cntrcs.txt'\n",
    "PREPROCESS = False\n",
    "TEST = False\n",
    "TEST_PATH = False\n",
    "MODEL = False\n",
    "MODEL_PATH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set argparse.\n",
    "parser = argparse.ArgumentParser(description='Convert sentences and paragraphs into a dense representation.')\n",
    "\n",
    "# Set logger.\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "file_handler = logging.FileHandler('logs/doc2vec.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set sentence classifier.\n",
    "sent_cls_path = 'sentence_classifier/classifiers/17-11-03_18:45/sentence_classifier_17-11-03_18:45.pkl'\n",
    "sent_cls_names_path = 'sentence_classifier/classifiers/17-11-03_18:45/sentence_classifier_dict_17-11-03_18:45.pkl'\n",
    "sent_cls = SentenceClassifier()\n",
    "sent_cls.load_classifier(sent_cls_path)\n",
    "sent_cls_names = pickle.load(open(sent_cls_names_path, 'r'))\n",
    "sent_cls.set_names(sent_cls_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.sentences = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        for uid, line in enumerate(open(self.filename)):\n",
    "            pred = sent_cls.predict_class(line)\n",
    "            if pred[0]:\n",
    "                yield TaggedDocument(words=line.split(), tags=['SENT_%s' % uid])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_path():\n",
    "\n",
    "    logger.info('Generating output path.')\n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    return 'models/model_' + strftime(\"%Y-%m-%d_%H-%M-%S.doc2vec\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(sentences, model=None):\n",
    "    logger.info('Training model.')\n",
    "\n",
    "    if not model:\n",
    "        model = Doc2Vec(size=100, window=2, min_count=2, workers=2, alpha=0.025, min_alpha=0.025)\n",
    "\n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.alpha -= 0.002  # decrease the learning rate\n",
    "        model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "    output_path = get_model_path()\n",
    "\n",
    "    logger.info('Saving trained model.')\n",
    "    model.save(output_path)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sent_dict(sentences):\n",
    "\n",
    "    s_dict = dict()\n",
    "\n",
    "    for sent in sentences:\n",
    "        s_dict[sent[1][0]] = sent[0]\n",
    "\n",
    "    return s_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "\n",
    "    file_path = TRAIN_PATH\n",
    "\n",
    "    logger.info('Receive training path: %s' % file_path)\n",
    "\n",
    "    # Get sentences.\n",
    "    if PREPROCESS:\n",
    "        logger.info('Preprocessing file.')\n",
    "        file_path = convert_to_sentences(file_path)\n",
    "\n",
    "    sentences = LabeledLineSentence(file_path)\n",
    "\n",
    "    # Create a dict to convert a sent code into its respective sentence.\n",
    "    sent_dict = create_sent_dict(sentences)\n",
    "\n",
    "    if not MODEL:\n",
    "        output_model = train_model(sentences)\n",
    "    else:\n",
    "        old_model = Doc2Vec.load(MODEL_PATH)\n",
    "        output_model = train_model(sentences, old_model)\n",
    "\n",
    "    base, _ = os.path.splitext(output_model)\n",
    "\n",
    "    # Save the dict.\n",
    "    pickle.dump(sent_dict, open(base + '.pkl', 'w'))\n",
    "\n",
    "elif TEST:\n",
    "    model = Doc2Vec.load(TEST_PATH)\n",
    "    # print model.docvecs.most_similar(20)\n",
    "    print model.infer_vector('This shall be respected.')\n",
    "\n",
    "else:\n",
    "    print \"Nothing to do here.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a new contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_classifier = 'sentence_classifier/classifiers/17-11-03_18:45/sentence_classifier_17-11-03_18:45.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_norms(path_to_classifier):\n",
    "    \n",
    "    norms = []\n",
    "    \n",
    "    return norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contract_path = ''\n",
    "\n",
    "# Read contract text.\n",
    "text = open(contract_path, 'r').read()\n",
    "\n",
    "# Extract sentences.\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Extract Norms.\n",
    "\n",
    "\n",
    "# Get norm representations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
